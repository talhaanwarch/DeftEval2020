{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deftEval_GloVe-GRU",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhaanwarch/DeftEval2020/blob/master/deftEval_GloVe_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ4QJP-XYRps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "52f80089-3cdc-433d-cb0d-6c2032bcdea3"
      },
      "source": [
        "!pip install keras_metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6UvuYsYnEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "a9797d0e-4eab-4f9b-fcf7-9e98e3ad7eb6"
      },
      "source": [
        "import keras_metrics as km"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgPHqUe8EPRa",
        "colab_type": "code",
        "outputId": "44034f27-95ee-412a-9083-c47ffb46dc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTD5-JmVEUHL",
        "colab_type": "code",
        "outputId": "73341419-46b5-481c-d585-f2a3879ea101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/dataset/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow_fcOEPEhad",
        "colab_type": "code",
        "outputId": "2f11d5de-4768-4a9d-9446-32c7429ba5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdeft_eval\u001b[0m/  \u001b[01;34mEEG\u001b[0m/  images.zip  \u001b[01;34mOLID\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4-p1JCELdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import string\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ajqEqUUE5Fx",
        "colab_type": "code",
        "outputId": "1c2bb19f-c494-4eb0-955a-4f7c7dfa71f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFkqtraEQLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_files = glob.glob('deft_eval/train/' + \"*.deft\")\n",
        "li = []\n",
        "for filename in train_files:\n",
        "    df = pd.read_csv(filename, sep='\\t',index_col=None, header=None,names=['sentence','label'])\n",
        "    li.append(df)\n",
        "\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "y_train=frame['label']\n",
        "corpus_train = []\n",
        "for j in frame['sentence']:\n",
        "    df=j.lower()\n",
        "    df=df.translate(str.maketrans('', '', string.punctuation))\n",
        "    df=df.replace( 'link ','')\n",
        "    df=''.join([i for i in df if not i.isdigit()]) \n",
        "    df=\" \".join(df.split())\n",
        "    df = [lemmatizer.lemmatize(token) for token in df.split(\" \")]\n",
        "    df=[word for word in df if not word in stop_words]\n",
        "    df=\" \".join(df)\n",
        "    corpus_train.append(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx272yuBFew0",
        "colab_type": "code",
        "outputId": "8709ea8b-ec57-42fc-9cf4-daa9565dfc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "len(corpus_train),corpus_train[0],y_train[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16659,\n",
              " 'science includes diverse field astronomy biology computer science geology logic physic chemistry mathematics',\n",
              " 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dvF59TxEztB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_files = glob.glob('deft_eval/dev/' + \"*.deft\")\n",
        "li = []\n",
        "for filename in dev_files:\n",
        "    df = pd.read_csv(filename, sep='\\t',index_col=None, header=None,names=['sentence','label'])\n",
        "    li.append(df)\n",
        "\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "y_dev=frame['label']\n",
        "corpus_dev = []\n",
        "for j in frame['sentence']:\n",
        "    df=j.lower()\n",
        "    df=df.translate(str.maketrans('', '', string.punctuation))\n",
        "    df=df.replace( 'link ','')\n",
        "    df=''.join([i for i in df if not i.isdigit()]) \n",
        "    df=\" \".join(df.split())\n",
        "    df = [lemmatizer.lemmatize(token) for token in df.split(\" \")]\n",
        "    df=[word for word in df if not word in stop_words]\n",
        "    df=\" \".join(df)\n",
        "    corpus_dev.append(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyvrnPA-Fozr",
        "colab_type": "code",
        "outputId": "dc3caef6-3f80-4805-d5c0-137c728ef87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "len(corpus_dev),corpus_dev[0],y_dev[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(810,\n",
              " 'becomes clear definition application scientific method play major role science',\n",
              " 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TadFtX9hY2jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84d2c992-e6ff-4063-dedd-9840daf5c756"
      },
      "source": [
        "import collections\n",
        "collections.Counter(y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 11090, 1: 5569})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xwg4b-EvUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "max_words = 10000 #frequency of words to be kept\n",
        "max_len = 300\n",
        "tokenize = Tokenizer(num_words=max_words)\n",
        "tokenize.fit_on_texts(corpus_train)\n",
        "sequences = tokenize.texts_to_sequences(corpus_train)\n",
        "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "word_index = tokenize.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnUCAjYAOOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "024e547d-a8de-48df-cb94-42ca66000a36"
      },
      "source": [
        "import os\n",
        "embeddings_index = {}\n",
        "f = open('OLID/glove.6B.100d.txt',encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwjJEiynAQbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a3cccbd-f85b-47e0-9520-91affcfe4ba9"
      },
      "source": [
        "num_words = min(max_words, len(word_index)) + 1\n",
        "print(num_words)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_index.items():\n",
        "    if i > max_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCP9ljoZKpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequences = tokenize.texts_to_sequences(corpus_dev)\n",
        "X_dev = sequence.pad_sequences(test_sequences,maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB8qW7jFGQlj",
        "colab_type": "code",
        "outputId": "69b8b456-82d9-4c86-e99f-314cf4366ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,CuDNNGRU,Dense,Dropout,Bidirectional,SpatialDropout1D,GlobalMaxPool1D\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from sklearn.utils import class_weight\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import keras_metrics as km\n",
        "\n",
        "from keras.initializers import Constant\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words,\n",
        "                    embedding_dim,\n",
        "                    embeddings_initializer=Constant(embedding_matrix),\n",
        "                    input_length=max_len,\n",
        "                    trainable=True))\n",
        "#model.add( SpatialDropout1D(0.2))\n",
        "#model.add((CuDNNGRU(50, return_sequences = True)))\n",
        "model.add((CuDNNGRU(30)))\n",
        "\n",
        "#model.add(GlobalMaxPool1D())\n",
        "#model.add(Dense(80, activation=\"sigmoid\"))\n",
        "model.add(Dense(30, activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(0.0001), metrics=[km.f1_score()])\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 100)          1000100   \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (None, 100)               60600     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 30)                3030      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 1,063,761\n",
            "Trainable params: 1,063,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG-PhyfzZIA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=5,restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-FFhG0iGYvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "dc639ce1-bf80-423d-93ae-65a953e1cc8e"
      },
      "source": [
        "model.fit(X_train,y_train,batch_size=30,epochs=50,verbose=2,class_weight=class_weights,validation_data=(X_dev,y_dev),callbacks=[es])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16659 samples, validate on 810 samples\n",
            "Epoch 1/50\n",
            " - 29s - loss: 0.6828 - f1_score: 0.4571 - val_loss: 0.6203 - val_f1_score: 0.5398\n",
            "Epoch 2/50\n",
            " - 26s - loss: 0.6313 - f1_score: 0.5576 - val_loss: 0.5882 - val_f1_score: 0.5900\n",
            "Epoch 3/50\n",
            " - 26s - loss: 0.5991 - f1_score: 0.5935 - val_loss: 0.5613 - val_f1_score: 0.5980\n",
            "Epoch 4/50\n",
            " - 26s - loss: 0.5741 - f1_score: 0.6182 - val_loss: 0.5493 - val_f1_score: 0.6146\n",
            "Epoch 5/50\n",
            " - 26s - loss: 0.5492 - f1_score: 0.6375 - val_loss: 0.5483 - val_f1_score: 0.6447\n",
            "Epoch 6/50\n",
            " - 26s - loss: 0.5314 - f1_score: 0.6558 - val_loss: 0.5137 - val_f1_score: 0.6275\n",
            "Epoch 7/50\n",
            " - 26s - loss: 0.5100 - f1_score: 0.6783 - val_loss: 0.5269 - val_f1_score: 0.6547\n",
            "Epoch 8/50\n",
            " - 26s - loss: 0.4893 - f1_score: 0.6911 - val_loss: 0.5030 - val_f1_score: 0.6478\n",
            "Epoch 9/50\n",
            " - 26s - loss: 0.4706 - f1_score: 0.7067 - val_loss: 0.5137 - val_f1_score: 0.6689\n",
            "Epoch 10/50\n",
            " - 26s - loss: 0.4525 - f1_score: 0.7234 - val_loss: 0.5156 - val_f1_score: 0.6689\n",
            "Epoch 11/50\n",
            " - 26s - loss: 0.4339 - f1_score: 0.7374 - val_loss: 0.5320 - val_f1_score: 0.6719\n",
            "Epoch 12/50\n",
            " - 26s - loss: 0.4159 - f1_score: 0.7498 - val_loss: 0.5068 - val_f1_score: 0.6500\n",
            "Epoch 13/50\n",
            " - 26s - loss: 0.3996 - f1_score: 0.7649 - val_loss: 0.5175 - val_f1_score: 0.6540\n",
            "Epoch 14/50\n",
            " - 26s - loss: 0.3803 - f1_score: 0.7763 - val_loss: 0.5511 - val_f1_score: 0.6731\n",
            "Epoch 15/50\n",
            " - 26s - loss: 0.3635 - f1_score: 0.7875 - val_loss: 0.5592 - val_f1_score: 0.6785\n",
            "Epoch 16/50\n",
            " - 25s - loss: 0.3469 - f1_score: 0.7996 - val_loss: 0.5528 - val_f1_score: 0.6841\n",
            "Epoch 17/50\n",
            " - 25s - loss: 0.3290 - f1_score: 0.8099 - val_loss: 0.5857 - val_f1_score: 0.6823\n",
            "Epoch 18/50\n",
            " - 25s - loss: 0.3136 - f1_score: 0.8215 - val_loss: 0.5716 - val_f1_score: 0.6643\n",
            "Epoch 19/50\n",
            " - 25s - loss: 0.2983 - f1_score: 0.8341 - val_loss: 0.5852 - val_f1_score: 0.6655\n",
            "Epoch 20/50\n",
            " - 25s - loss: 0.2854 - f1_score: 0.8427 - val_loss: 0.5940 - val_f1_score: 0.6724\n",
            "Epoch 21/50\n",
            " - 25s - loss: 0.2689 - f1_score: 0.8516 - val_loss: 0.6198 - val_f1_score: 0.6922\n",
            "Epoch 22/50\n",
            " - 25s - loss: 0.2538 - f1_score: 0.8617 - val_loss: 0.6215 - val_f1_score: 0.6875\n",
            "Epoch 23/50\n",
            " - 25s - loss: 0.2436 - f1_score: 0.8713 - val_loss: 0.6419 - val_f1_score: 0.6863\n",
            "Epoch 24/50\n",
            " - 25s - loss: 0.2324 - f1_score: 0.8782 - val_loss: 0.6604 - val_f1_score: 0.6811\n",
            "Epoch 25/50\n",
            " - 25s - loss: 0.2206 - f1_score: 0.8851 - val_loss: 0.6716 - val_f1_score: 0.6567\n",
            "Epoch 26/50\n",
            " - 25s - loss: 0.2103 - f1_score: 0.8916 - val_loss: 0.6852 - val_f1_score: 0.6875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f35af316c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QJVrux7X_HK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ffb51115-25b1-4565-b273-9db08a2a83fb"
      },
      "source": [
        "\n",
        "print(model.evaluate(X_dev,y_dev))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "810/810 [==============================] - 0s 561us/step\n",
            "[0.6198218173450893, 0.6921796506875709]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw_7KyuTYoVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "b571df86-ee88-4323-f0ba-bb05c277b211"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_dev, batch_size=30, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "print(classification_report(y_dev, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "810/810 [==============================] - 0s 604us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.78      0.82       537\n",
            "           1       0.63      0.76      0.69       273\n",
            "\n",
            "    accuracy                           0.77       810\n",
            "   macro avg       0.75      0.77      0.76       810\n",
            "weighted avg       0.79      0.77      0.78       810\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1muzKvLY8IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwIa9qjzc23d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ-D5Tf2YQXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZiTzteAqwAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}